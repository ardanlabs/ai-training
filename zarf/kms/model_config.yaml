# example-model:
#   context-window: 8192      # Max tokens model can process (default: 8192)
#   nbatch: 2048              # Logical batch size (default: 2048)
#   nubatch: 512              # Physical batch size for prompt ingestion (default: 512)
#   nthreads: 0               # Threads for generation (0 = llama.cpp default)
#   nthreads-batch: 0         # Threads for batch processing (0 = llama.cpp default)
#   cache-type-k: q8_0        # KV cache key type: f32, f16, q8_0, q4_0, bf16, auto
#   cache-type-v: q8_0        # KV cache value type: f32, f16, q8_0, q4_0, bf16, auto
#   flash-attention: enabled  # Flash Attention: enabled, disabled, auto
#   device: ""                # Device to use (run llama-bench --list-devices)
#   nseq-max: 0               # Max parallel sequences for batched inference (0 = default)
#   offload-kqv: true         # Offload KV cache to GPU (false = keep on CPU)
#   op-offload: true          # Offload tensor operations to GPU (false = keep on CPU)
#   ngpu-layers: 0            # GPU layers to offload (0 = all, -1 = none, N = specific count)

Qwen3-8B-Q8_0:
  context-window: 32768
  cache-type-k: f16
  cache-type-v: f16
  nbatch: 1024
  nubatch: 256
  flash-attention: enabled
  nseq-max: 2
Qwen2.5-VL-3B-Instruct-Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 2048
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
Qwen2-Audio-7B.Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 2048
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
embeddinggemma-300m-qat-Q8_0:
  context-window: 2048
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
