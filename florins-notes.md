---------
example 5: Should we first show the `llm.Call` without the Streaming func so we can teach the difference between streaming of response and waiting the full response?

Open question: Is it better for us to wait for the LLM to return the full response in some cases? Or streaming always works? What about tool calling?

---------

example 5+: All `llm.Call` results are called synchronously. For code simplicity, we should remove the WaitGroup

---------

example 6/9: I think the current model generates only 512 values for the embeddings vectors, not 1024.
We should probably reduce the number we use in `NumDimensions` to 512 in this case.

Open questions:
What's the size of the embedding generated by the current model?
What about other models?
Are all embeddings generated by a model always guaranteed to be the same length?

---------

example 7: Should we add the following score sorting?
sort.Slice(results, func(i, j int) bool {
	return res[i].Score > res[j].Score
})

---------

example 7:

Open questions:
Why do we need only 1000 words? For speed optimization/token count/accuracy?
And, since we say that 1000 words limit, then shouldn't the higher score result be included first?
See the previous comment.

---------

example 10: step 1:

Open questions:
Why do the OLLAMA_CONTEXT_LENGTH check/increase in the `init()` call and not in the `run()` call?
I thought the temperature should be at least 0.1 not 0.0.
Should we add a `/bye` message to nicely terminate the app? I know it's training material, but we can still be nice.
Can `resp.Choices` `resp.Choices[0]` `resp.Choices[0].Delta` ever be nil?

Should we introduce the different types of prompts earlier? It's the first time we have the `user` and `assistant`
prompt types shown.
Should we demonstrate what a system prompt is and how to use one? The sliding window for messages to not reach the
maximum configured for messages should always start with the system prompt then the messages should be captured in
the sliding window.


For tool calling, different models have different templates for tool calling. We should look into Ollama and
how it handles this case. Even if it does automatically provide a template for the tool calling, we should still
teach people about tool calling templates and how to use them (Florin: show example of Llama tool calling template).

---------

example 10 step 2:

Discuss the fact that models like and work better with structured data.
That's why we should have the JSON output. Should this discussion be better in a previous example? Which one?

Should we move the GetWeather tool in another file, e.g. `weather_tool.go` or similar such that the code in main
remains a bit more focused, and we define tools in their own "spaces"?

---------

Foundation -> client -> client.go

Why does `func (cln *SSEClient[T]) Do(ctx context.Context, method string, endpoint string, body D, ch chan T) error`
accept a channel rather than return it as a read-only channel?
Shouldn't the function using the channel also manage its state (open for messages/closed) in this case?