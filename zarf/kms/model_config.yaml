# example-model:
#   context-window: 8192                 # Max tokens model can process (default: 8192)
#   nbatch: 2048                         # Logical batch size (default: 2048)
#   nubatch: 512                         # Physical batch size for prompt ingestion (default: 512)
#   nthreads: 0                          # Threads for generation (0 = llama.cpp default)
#   nthreads-batch: 0                    # Threads for batch processing (0 = llama.cpp default)
#   cache-type-k: q8_0                   # KV cache key type: f32, f16, q8_0, q4_0, bf16, auto
#   cache-type-v: q8_0                   # KV cache value type: f32, f16, q8_0, q4_0, bf16, auto
#   flash-attention: enabled             # Flash Attention: enabled, disabled, auto
#   device: ""                           # Device to use (run llama-bench --list-devices)
#   nseq-max: 0                          # Max parallel sequences for batched inference (0 = default)
#   offload-kqv: true                    # Offload KV cache to GPU (false = keep on CPU)
#   op-offload: true                     # Offload tensor operations to GPU (false = keep on CPU)
#   ngpu-layers: 0                       # GPU layers to offload (0 = all, -1 = none, N = specific count)
#   split-mode: row                      # Multi-GPU split: none, layer, row (row recommended for MoE models)
#   system-prompt-cache: false           # Cache system prompt KV state for reuse across requests
#   incremental-cache: false             # Incremental message caching for agentic workflows (Cline, OpenCode)
#   max-imc-sessions: 1                  # Max concurrent IMC sessions/users (default: 1, each gets own cache)
#   cache-min-tokens: 100                # Min tokens before caching (default: 100, applies to both cache types)
#   rope-scaling-type: yarn              # RoPE scaling: none, linear, yarn
#   yarn-orig-ctx: 32768                 # Original training context size (nil = from model)
#   rope-freq-base: 1000000              # RoPE base frequency (nil = from model, e.g., 10000 Llama, 1000000 Qwen)
#   rope-freq-scale: 0.25                # RoPE frequency scale (nil = auto-calculated)
#   yarn-ext-factor: 1.0                 # YaRN extrapolation mix factor (nil = auto, 0 = disable)
#   yarn-attn-factor: 1.0                # YaRN attention magnitude scaling (nil = default 1.0)
#   yarn-beta-fast: 32.0                 # YaRN low correction dimension (nil = default 32.0)
#   yarn-beta-slow: 1.0                  # YaRN high correction dimension (nil = default 1.0)

Qwen3-8B-Q8_0:
  context-window: 32768
  cache-type-k: f16
  cache-type-v: f16
  nbatch: 1024
  nubatch: 256
  flash-attention: enabled
  nseq-max: 2

Qwen2.5-VL-3B-Instruct-Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 2048
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled

Qwen2-Audio-7B.Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 2048
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled

embeddinggemma-300m-qat-Q8_0:
  context-window: 2048
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
